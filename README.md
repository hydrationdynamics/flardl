# Adaptive, Elastic Multi-Server Downloading

[![PyPI](https://img.shields.io/pypi/v/flardl.svg)][pypi status]
[![Python Version](https://img.shields.io/pypi/pyversions/flardl)][pypi status]
[![Docs](https://img.shields.io/readthedocs/flardl/latest.svg?label=Read%20the%20Docs)][read the docs]
[![Tests](https://github.com/hydrationdynamics/flardl/workflows/Tests/badge.svg)][tests]
[![Codecov](https://codecov.io/gh/hydrationdynamics/flardl/branch/main/graph/badge.svg)][codecov]
[![Repo](https://img.shields.io/github/last-commit/hydrationdynamics/flardl)][repo]
[![Downloads](https://pepy.tech/badge/flardl)][downloads]
[![Dlrate](https://img.shields.io/pypi/dm/flardl)][dlrate]
[![Codacy](https://app.codacy.com/project/badge/Grade/5d86ff69c31d4f8d98ace806a21270dd)][codacy]
[![Snyk Health](https://snyk.io/advisor/python/flardl/badge.svg)][snyk]

[pypi status]: https://pypi.org/project/flardl/
[read the docs]: https://flardl.readthedocs.io/
[tests]: https://github.com/hydrationdynamics/flardl/actions?workflow=Tests
[codecov]: https://app.codecov.io/gh/hydrationdynamics/flardl
[repo]: https://github.com/hydrationdynamics/flardl
[downloads]: https://pepy.tech/project/flardl
[dlrate]: https://github.com/hydrationdynamics/flardl
[codacy]: https://www.codacy.com/gh/hydrationdynamics/flardl?utm_source=github.com&utm_medium=referral&utm_content=hydrationdynamics/zeigen&utm_campaign=Badge_Grade
[snyk]: https://snyk.io/advisor/python/flardl

> Who would flardls bear?

[![logo](https://raw.githubusercontent.com/hydrationdynamics/flardl/main/docs/_static/flardl_bear.png)][logo license]

[logo license]: https://raw.githubusercontent.com/hydrationdynamics/flardl/main/LICENSE.logo.txt

## Features

*Flardl* downloads lists of files using an approach that
adapts to local conditions and is elastic with respect
to changes in network performance and server loads.
*Flardl* achieves download rates **typically more than
300X higher** than synchronous utilities such as *curl*,
while use of multiple servers while providing superior
robustness and protection against blacklisting for
installations anywhere in the world. Download rates depend
on network bandwidth, latencies, list length, file sizes,
and HTTP protocol used, but even a single server on another
continent can usually saturate a gigabit cable connection
after about 50 files.

## Long-Tailed Size Distributions

Collections of files generated by natural or human activity such
as natural-language writing, protein structure determination,
or genome sequencing tend to have **size distributions with
long tails**. Such distributions have more big files than
small files at a given size above or below the peak (modal) value.
Analytical forms of long-tail distributions include Zipf, power-law,
and log-norm distributions. A real-world example of a long-tail
distribution is shown below, which plots the file-size histogram
for 1000 randomly-sampled CIF structure files from the
[Protein Data Bank](https://rcsb.org),
along with a kernel-density estimate and fits to log-normal and
normal distributions.

![sizedist](https://raw.githubusercontent.com/hydrationdynamics/flardl/main/docs/_static/file_size_distribution.png)

Algorithms that use per-file rates alone and ignore the effects
of big files on overall download statistics are implicitly
assuming the normal fits above. One issue can be seen in the
normal-distribution fits above is that **mean values are neither
stable nor characteristic of the distribution**. Fiting a
normal distribution to 5% of the data give a markedly-lower
mean and standard deviation than the fit to all points,
The means of runs drawn from long-tail distributions tend to
grow larger as more files downloaded because the more files
in the list, the higher the likelihood that one or more of
them will be huge. Even though the shape of the distribution
doesn't change with number of files in the download list, your
sampling of the distribution is discrete (i.e., you either get
a huge file or you don't). Downloading algorithms that rely on
total time or average per-file rates will give poor performance,
either through launching requests too slowly or through letting
queues run too deep when big files are encountered.

While the *mean* per-file download rate isn't a good statistic, the
*most-common* per-file download rate $k_{\rm file}$ can be more
consistent, at least on timescale over which network performance
and server loads are consistent. It's not a surprise that downloads
can take longer if you are watching a video at the same time. The
slowing is usually not because your computer runs out of processor
bandwidth, but because your LAN has to be shared between two demand
stream. Higher predictability and performance can be achieved by
breaking down the modal per-file rate $\tilde{k}_{\rm file}$ into
modal file size $\tilde{S}$, assumed highly stable over time, and
the achievable bit rate of your LAN, $B_{\rm act}$, which is often
near the maximum rate your LAN vendor sold you $B_{\rm max}$, but
can be reduced due to demand from competing uses.

## Avoiding Blacklists

Even more than maximizing download rates, the highest priority must
be to **avoid black-listing by a server**. Most public-facing servers
have policies to recognize and defend against Denial-Of-Service (DOS)
attacks. The response to a DOS event, at the very least, causes the
server to dump your latest request, which is usually a minor nuisance
as it can be retried later. Far worse is if the server responds by
severely throttling further requests from your IP address for hours
or sometime days. Worst of all, your IP address can get the "death
penalty" and be put on a permanent blacklist that may require manual
intervention for removal. You generally don't know thThe simplest
possibility of le trigger levels for these policies. Blacklisting
might not even be your personal fault, but a collective problem.
I have seen a practical class of 20 students brought to a complete
halt by a server's 24-hour black-listing of the institution's
public IP address.

## Fishing Theory

An analogy might help us here. Let's say you are a person who
enjoys keeping track of statistics, and you decide to try
fishing. At first, you have a single fishing rod and you go
fishing at a series of local lakes where your catch consists
of small bony fishes called "crappies". Your records reval
that while the rate of catching fishes can vary from day to
day--fish might be hungry or not--the average size of your
catch is pretty stable. Bigger ponds tend to have bigger fish
in them, and it might take slightly longer to reel in a bigger
crappie than a small one, but big and small averages out to
that pond.

Then one day you decide you love fishing so much, you drive
to the coast and charter a fishing boat. On that boat,
you can set out as many lines as you want (up to some limit)
and fish in parallel. At first, you seem to be catching the
ocean-going equivalent of crappies, small bony fishes. But
then you hook a small shark, which not only takes a lot of
your time and attention to reel in, but which totally skews
your estimate of average weight of your catch. You know that
if you can catch a small shark, then maybe if you fish for
long enough you might catch a big shark, or even a small whale.
But you and your crew can only effecively reel in so
many hooked lines at once. Putting out more lines than
that effective limit of hooked- plus waiting-to-be-hooked
lines only results in fishes waiting on the line, when they
may break the line or get partly eaten before you can reel
them in.

## Adaptilastic Queueing

_Flardl_ implements a method I call "adaptilastic"
queueing to deliver robust performance in real situations,
while being simple enough to be easily understood and coded.
The basis of edaptilastic queueing is setting the total
request-queue depth, across all servers, just high enough
to saturate the total downloading bit rate. On startup,
_flardl_ launches requests at all servers the most-likely
per-file rate at saturation, up to some maximum permissible
per-server queue depth $D_{\rm max_{i}}$ (set either by guess or
by previous knowledge of individual servers). As transfers are
completed, _flardl_ estimates the depth at which saturation
was achieved (totalled over all servers), and updates its
estimate of the achievable line bit rate and the most-likely
per-file return rate on a per-server basis. These values form
the bases for launching the remaining requests. The servers
with higher modal service rates (i.e., rates of serving
crappies) will spend less time waiting and thus stand a better
chance at nabbing an open queue slot, without penalizing servers
that happen to draw a big downloads (whales).

## Requirements

_Flardl_ is tested under python 3.11, on Linux, MacOS, and
Windows and under 3.9 and 3.10 on Linux. Under the hood,
_flardl_ relies on [httpx](https://www.python-httpx.org/) and is supported
on whatever platforms that library works under, for both HTTP/1.1
and HTTP/2.

## Installation

You can install _Flardl_ via [pip] from [PyPI]:

```console
$ pip install flardl
```

## Usage

_Flardl_ has no CLI and does no I/O other than downloading and writing
files. See test examples for usage.

## Contributing

Contributions are very welcome.
To learn more, see the [Contributor Guide].

## License

Distributed under the terms of the [BSD 3-clause_license][license],
_Flardl_ is free and open source software.

## Issues

If you encounter any problems,
please [file an issue] along with a detailed description.

## Credits

_Flardl_ was written by Joel Berendzen.

[pypi]: https://pypi.org/
[file an issue]: https://github.com/hydrationdynamics/flardl/issues
[pip]: https://pip.pypa.io/

<!-- github-only -->

[license]: https://github.com/hydrationdynamics/flardl/blob/main/LICENSE
[contributor guide]: https://github.com/hydrationdynamics/flardl/blob/main/CONTRIBUTING.md
